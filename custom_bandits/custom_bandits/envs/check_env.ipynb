{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import gym\r\n",
    "from gym import spaces\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class TwoArmBandit(gym.Env):\r\n",
    "    \r\n",
    "    # metadata = {'render.modes':['human']}\r\n",
    "    \r\n",
    "\r\n",
    "    def __init__(self, alpha, beta):\r\n",
    "        super(TwoArmBandit, self).__init__()\r\n",
    "        N_DISCRETE_ACTIONS = 2\r\n",
    "        LEFT = 0\r\n",
    "        RIGHT = 1\r\n",
    "        N_DISCRETE_STATES = 3\r\n",
    "        self.alpha = alpha\r\n",
    "        self.beta = beta\r\n",
    "        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\r\n",
    "        self.observation_space = spaces.Discrete(N_DISCRETE_STATES)\r\n",
    "        \r\n",
    "        # Data structure to store MDP of 2-arm Bernoulli Bandit\r\n",
    "        self.P = {}\r\n",
    "        self.P[0] = {\r\n",
    "                        LEFT: [[self.alpha, 1, 0, True], [1-self.alpha, 2, 1, True]],\r\n",
    "                        RIGHT: [[self.beta, 2, 1, True], [1-self.beta, 1, 0, True]]\r\n",
    "                    }\r\n",
    "        self.P[1] = {\r\n",
    "                        LEFT: [[1,1,0,True]],\r\n",
    "                        RIGHT: [[1,1,0,True]]\r\n",
    "                    }\r\n",
    "        self.P[2] = {\r\n",
    "                        LEFT: [[1,2,0,True]],\r\n",
    "                        RIGHT: [[1,2,0,True]]\r\n",
    "                    }\r\n",
    "        self.agent_position = self.reset()\r\n",
    "\r\n",
    "    def step(self, action):\r\n",
    "        # get experience tuple from MDP dynamics\r\n",
    "\r\n",
    "        probabilities = [] # to collect probabilities of various states agent can land in\r\n",
    "        for dynamic in self.P[self.agent_position][action]: # collecting probabilities\r\n",
    "            probabilities.append(dynamic[0])\r\n",
    "\r\n",
    "        idx = [i for i in range(len(self.P[self.agent_position][action]))] # indices to choose from ie number of tuples present in MDP corresponding to current state and action taken\r\n",
    "        j = int(np.random.choice(idx,1,probabilities)) # select where to go according to the probablities\r\n",
    "        \r\n",
    "        _, observation, reward, done = self.P[self.agent_position][action][j] # collect next experience tuple\r\n",
    "        \r\n",
    "        # update agent's position\r\n",
    "        self.agent_position = observation \r\n",
    "        info = {}\r\n",
    "        return observation, reward, done, info\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        self.agent_position = 0\r\n",
    "        return self.agent_position  # reward, done, info can't be included\r\n",
    "    \r\n",
    "    def render(self, mode='human'):\r\n",
    "        raise NotImplementedError\r\n",
    "\r\n",
    "    def close (self):\r\n",
    "        raise NotImplementedError"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "class TenArmGaussianBandit(gym.Env):\r\n",
    "    \r\n",
    "    # metadata = {'render.modes':['human']}\r\n",
    "    \r\n",
    "\r\n",
    "    def __init__(self, mu = 0, sigma_square=1, seed=0):\r\n",
    "        super(TenArmGaussianBandit, self).__init__()\r\n",
    "        N_DISCRETE_ACTIONS = 10\r\n",
    "        N_DISCRETE_STATES = 11\r\n",
    "        \r\n",
    "        self.seed(seed)\r\n",
    "\r\n",
    "        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\r\n",
    "        self.observation_space = spaces.Discrete(N_DISCRETE_STATES)\r\n",
    "        \r\n",
    "        # sample from a gaussian 10 times to create the 10 arm gaussian bandit\r\n",
    "        self.mu = mu\r\n",
    "        self.sigma_square = sigma_square\r\n",
    "        self.q_value = np.random.normal(self.mu, np.sqrt(self.sigma_square), self.action_space.n)\r\n",
    "        self.rewards = np.random.normal(self.q_value, np.sqrt(self.sigma_square), self.action_space.n)\r\n",
    "\r\n",
    "        self.P = self.set_MDP()\r\n",
    "        self.agent_position = self.reset()\r\n",
    "\r\n",
    "    def set_MDP(self):\r\n",
    "        print(self.rewards)\r\n",
    "        P = {}\r\n",
    "        for i in range(0,self.observation_space.n):\r\n",
    "            P[i] = {}\r\n",
    "        for i in range(0,self.action_space.n):\r\n",
    "            P[0][i] = [(1,i+1,self.rewards[i],True)]\r\n",
    "        \r\n",
    "        for i in range(1,self.observation_space.n):\r\n",
    "            for j in range(0,self.action_space.n):\r\n",
    "                P[i][j] = [(1,i,0,True)]\r\n",
    "        return P\r\n",
    "    \r\n",
    "    def step(self, action):\r\n",
    "        if self.agent_position != 0:\r\n",
    "            return self.agent_position, 0, True, {}\r\n",
    "        else:\r\n",
    "            self.rewards = np.random.normal(self.q_value, np.sqrt(self.sigma_square), self.action_space.n)\r\n",
    "            self.set_MDP()\r\n",
    "            self.agent_position = action+1\r\n",
    "            reward = self.rewards[action]\r\n",
    "            done = True\r\n",
    "            info = {}\r\n",
    "            return self.agent_position, reward, done, info\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        self.agent_position = 0\r\n",
    "        # self.rewards = np.random.normal(self.q_value, np.sqrt(self.sigma_square), self.action_space.n)\r\n",
    "        # self.set_MDP()\r\n",
    "        return self.agent_position  # reward, done, info can't be included\r\n",
    "    \r\n",
    "    def seed(self, seed=0):\r\n",
    "        np.random.seed(seed)\r\n",
    "\r\n",
    "    def render(self, mode='human'):\r\n",
    "        raise NotImplementedError\r\n",
    "\r\n",
    "    def close (self):\r\n",
    "        raise NotImplementedError"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class RandomWalk(gym.Env):\r\n",
    "    def __init__(self, alpha=0.5, beta=0.5, seed=0):\r\n",
    "        super(RandomWalk, self).__init__()\r\n",
    "        self.seed(seed)\r\n",
    "        N_DISCRETE_ACTIONS = 2\r\n",
    "        LEFT = 0\r\n",
    "        RIGHT = 1\r\n",
    "        N_DISCRETE_STATES = 7\r\n",
    "        self.alpha = alpha\r\n",
    "        self.beta = beta\r\n",
    "        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\r\n",
    "        self.observation_space = spaces.Discrete(N_DISCRETE_STATES)\r\n",
    "        self.agent_position = self.reset()\r\n",
    "\r\n",
    "    def step(self, action):\r\n",
    "        if self.agent_position == 0 or self.agent_position == 6:\r\n",
    "            return self.agent_position, 0, True, {}\r\n",
    "        else:\r\n",
    "            if action == 0:\r\n",
    "                if np.random.uniform() < self.alpha:\r\n",
    "                    self.agent_position -= 1 \r\n",
    "                    reward = 0\r\n",
    "                    done = False\r\n",
    "                    if self.agent_position == 0:\r\n",
    "                        done = True\r\n",
    "                else:\r\n",
    "                    self.agent_position += 1\r\n",
    "                    reward = 0\r\n",
    "                    done = False\r\n",
    "                    if self.agent_position == 6:\r\n",
    "                        reward = 1\r\n",
    "                        done = True\r\n",
    "            if action == 1:\r\n",
    "                if np.random.uniform() < self.beta:\r\n",
    "                    self.agent_position += 1 \r\n",
    "                    reward = 0\r\n",
    "                    done = False\r\n",
    "                    if self.agent_position == 0:\r\n",
    "                        done = True\r\n",
    "                else:\r\n",
    "                    self.agent_position -= 1\r\n",
    "                    reward = 0\r\n",
    "                    done = False\r\n",
    "                    if self.agent_position == 6:\r\n",
    "                        reward = 1\r\n",
    "                        done = True\r\n",
    "        info = {}\r\n",
    "        return self.agent_position, reward, done, info\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        self.agent_position = int(np.random.randint(1,6,1)[0])\r\n",
    "        return self.agent_position  \r\n",
    "    \r\n",
    "    def seed(self, seed=0):\r\n",
    "        np.random.seed(seed)\r\n",
    "\r\n",
    "    def render(self, mode='human'):\r\n",
    "        raise NotImplementedError\r\n",
    "\r\n",
    "    def close (self):\r\n",
    "        raise NotImplementedError"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "env = TenArmGaussianBandit(seed=0)\r\n",
    "for i in range(10):\r\n",
    "    env.reset()\r\n",
    "    action = 0\r\n",
    "    obs, reward, done, _ = env.step(action)\r\n",
    "    print(obs, env.q_value[action], reward, done)\r\n",
    "from stable_baselines.common.env_checker import check_env\r\n",
    "check_env(env)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 1.764052345967664 0.7154993809005714 True\n",
      "1 1.764052345967664 1.091591898191713 True\n",
      "1 1.764052345967664 0.5989025051843075 True\n",
      "1 1.764052345967664 3.6472030430239184 True\n",
      "1 1.764052345967664 2.1404778771232933 True\n",
      "1 1.764052345967664 0.2727947532620585 True\n",
      "1 1.764052345967664 1.2660198952753592 True\n",
      "1 1.764052345967664 1.41005843471418 True\n",
      "1 1.764052345967664 1.3948705080252204 True\n",
      "1 1.764052345967664 0.808107345474887 True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "\r\n",
    "class policyEvaluation:\r\n",
    "\r\n",
    "    def __init__(self, policy, gamma=0.99, theta=1e-10, max_iterations=500):\r\n",
    "        # policy distn\r\n",
    "        self.pi = policy\r\n",
    "        \r\n",
    "        # taking gamma as 0.99\r\n",
    "        self.gamma = 0.99\r\n",
    "\r\n",
    "        # threshold\r\n",
    "        self.theta = 1e-3\r\n",
    "\r\n",
    "        # max_iterations\r\n",
    "        self.max_iterations = max_iterations\r\n",
    "\r\n",
    "    def evaluate(self, env):\r\n",
    "        # randomly initialize old Value estimates, here initializing to 0\r\n",
    "        Vold = np.zeros(env.observation_space.n)\r\n",
    "        \r\n",
    "        for i in range(self.max_iterations):\r\n",
    "            Vnew = np.zeros(env.observation_space.n)\r\n",
    "            for s in range(env.observation_space.n): # for all states\r\n",
    "                for a in range(env.action_space.n): # for all actions in each state\r\n",
    "                    temp = 0\r\n",
    "                    \r\n",
    "                    for p,s_,r,d in env.P[s][a]: # for all dynamics \r\n",
    "                        # inner summation over next state and reward\r\n",
    "                        if not d:\r\n",
    "                            temp += p*(r+self.gamma*Vold[s_])\r\n",
    "                        else:\r\n",
    "                            temp += p*r\r\n",
    "                            # print('state:', s, 'action:', a, p, s_, 'reward:', r)\r\n",
    "\r\n",
    "                    Vnew[s] += self.pi[a]*temp # outermost summation over policy\r\n",
    "                    \r\n",
    "            if np.max(np.abs(Vnew-Vold)) < self.theta:\r\n",
    "                break \r\n",
    "            Vold = Vnew\r\n",
    "\r\n",
    "        for i in range(len(Vnew)):\r\n",
    "            if i==0:\r\n",
    "                print(f'    Value of initial state {i} is {np.round(Vnew[i],2)}')\r\n",
    "            else:\r\n",
    "                print(f'    Value of terminal state {i} is {np.round(Vnew[i],2)}')\r\n",
    "        return Vnew\r\n",
    "    \r\n",
    "    def __repr__(self):\r\n",
    "        return 'policyEvaluation(policy={}, gamma={}, theta={}, max_iterations={})'.format(self.policy, self.gamma, self.theta, self.max_iterations)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "if __name__ == '__main__':\r\n",
    "\r\n",
    "    env = TenArmGaussianBandit(sigma_square=1,seed=1)\r\n",
    "    env.reset()\r\n",
    "    # print(env.q_value)\r\n",
    "    a = np.argmax(env.q_value)\r\n",
    "    print(a)\r\n",
    "    some_policy = np.zeros(env.action_space.n)\r\n",
    "    some_policy[a] = 1\r\n",
    "    print(some_policy)\r\n",
    "    policyEvluator = policyEvaluation(some_policy)\r\n",
    "    policyEvluator.evaluate(env)\r\n",
    "    # print(env.rewards)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 3.0864533  -2.67189712 -0.85058896 -1.45702298  1.99917707 -3.40142996\n",
      "  1.57238356 -1.63906532  0.36125284  0.33344484]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "    Value of initial state 0 is 1.57\n",
      "    Value of terminal state 1 is 0.0\n",
      "    Value of terminal state 2 is 0.0\n",
      "    Value of terminal state 3 is 0.0\n",
      "    Value of terminal state 4 is 0.0\n",
      "    Value of terminal state 5 is 0.0\n",
      "    Value of terminal state 6 is 0.0\n",
      "    Value of terminal state 7 is 0.0\n",
      "    Value of terminal state 8 is 0.0\n",
      "    Value of terminal state 9 is 0.0\n",
      "    Value of terminal state 10 is 0.0\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('pytorch_env': conda)"
  },
  "interpreter": {
   "hash": "c833507e7eb7af7ca95c7423b09e2995fbfb15583771793a8991c1685b565bec"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}